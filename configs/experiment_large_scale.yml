# Configuration for large-scale system experiments

# Model Architecture - efficient for high dimensions
model:
  n_experts: 8
  expert_architecture:
    depth: 3  # Shallower for efficiency
    width: 512  # Wider networks
    activation: "tanh"  # Better for continuous dynamics (from quick_test)
    residual: true
    dropout: 0.0
  gating_architecture:
    depth: 2
    width: 256
    activation: "relu"
  history_embedding:
    type: "lstm"
    hidden_dim: 128
    num_layers: 1
  temperature: 0.3  # Lower temperature for sharper routing (from quick_test)
  expert_threshold: 0.2  # Higher threshold to force ~2 active experts
  # Initialization settings (from quick_test)
  use_improved_init: true
  expert_init_strategy: "mixed"
  gating_init_strategy: "uniform"

# Training Configuration - optimized for large systems
training:
  batch_size: 256  # Larger batch for stability (balanced with memory)
  learning_rate: 3e-4  # Optimal learning rate from quick_test
  num_epochs: 150
  early_stopping_patience: 20
  gradient_clip_norm: 5.0  # Less restrictive (from quick_test)
  sequence_length: 100
  optimizer: "adam"
  scheduler:
    type: "cosine"  # Better than step (from quick_test)
    warmup_epochs: 10
    min_lr: 1e-6
  regularization:
    route_weight: 0.1  # Moderate routing penalty (from quick_test)
    expert_weight: 1e-6  # Very light L2 (from quick_test)
    diversity_weight: 0.0  # No diversity penalty - let experts specialize
    smoothness_weight: 0.01  # Small smoothness penalty for energy
    balance_weight: 0.0  # Natural specialization

# Integration Configuration
integration:
  method: "dopri5"  # Better accuracy (from quick_test)
  rtol: 1e-3
  atol: 1e-4
  max_step_size: 0.1
  min_step_size: 1e-4
  adaptive_step: false  # Fixed step for speed
  routing_aware_step: false
  # Stability control
  dynamics_max_norm: 50.0  # From quick_test

# Data Configuration - high-dimensional systems
data:
  synthetic_systems:
    - name: "kuramoto_model"
      enabled: true
      n_trajectories: 10000  # More data (from quick_test approach)
      trajectory_length: 100
      sampling_dt: 0.01
      params:
        n_oscillators: [20, 50]
        coupling_strength: [1.0, 4.0]
  noise:
    observation_noise: 0.01
    process_noise: 0.001  # No process noise (from quick_test)
  train_val_test_split: [0.6, 0.2, 0.2]  # From quick_test
  temporal_sampling: "uniform"
  augmentation:
    random_rotation: true
    random_scaling: true  # Simpler augmentation

# Computational Resources
compute:
  device: "cuda"
  mixed_precision: false  # Disable for stability (from quick_test)
  gradient_checkpointing: false  # Simpler setup
  multi_gpu: false
  num_workers: 4  # Moderate workers

# Logging
logging:
  log_dir: "./logs_large_scale"
  checkpoint_dir: "./checkpoints_large_scale"
  tensorboard: true
  wandb:
    enabled: false
  save_frequency: 5
  log_frequency: 10

# Evaluation - comprehensive metrics
evaluation:
  metrics:
    - "trajectory_mse"
    - "computational_efficiency"
    - "expert_specialization"
    - "long_term_stability"
    - "phase_space_geometry"
    - "energy_conservation"
    - "routing_stability"
  visualization:
    phase_portraits: false  # Skip for high-dim
    routing_heatmaps: true
    trajectory_decomposition: false